Geminiプロジェクト v5: Pose + Clump ハイブリッド（最終案）

このドキュメントは、物体検出モデル（モデルB：プレート群）と汎用姿勢推定モデル（Pose）を組み合わせた、最終的な分析システムの開発手順書です。

フェーズ1：モデル学習（モデルB：プレート群）

目的: weight_clump（重りの塊）のみを学習させた、安定した物体検出モデル（モデルB）を作成済みであることを確認する。

1.1 学習コマンドの実行 (完了済み)

「yolo11n物体検出モデルの学習を開始するターミナルコマンドを提示してください。

dataファイルは my_data_B.yaml (単一クラス weight_clump が定義されたもの)

project (プロジェクト名) は Model_B_Clump

modelは yolo11n.pt

epochsは 100

imgszは 640
とします。」

フェーズ2：分析ロジックの開発 (Pose + Clump)

目的: 学習済みのモデルBと、汎用の姿勢推定モデルを組み合わせ、面積フィルタと距離フィルタを搭載した最強の軌道分析ロジックを開発する。

2.1 軌道データ抽出ロジックの作成 (最終案)

「以下の2つのモデルを入力として受け取るPythonスクリプト analyze_lift_final.py を作成してください。

モデルBのbest.pt (Model_B_Clump/train/weights/best.pt)

汎用姿勢推定モデル (yolo11m-pose.pt)

スクリプトは動画の全フレームを処理します。
各フレームで以下の処理を行ってください。

姿勢推定: yolo11m-pose.ptを使い、フレーム内の人間を検出し、そのキーポイント（肩と腰）から**「リフターの中心点」**を計算する。

物体検出: Model_B_Clumpを使い、フレーム内の**全てのweight_clump**を検出する。（背景の誤検出も含む）

フィルタリング（ハイブリッド）:

まず、検出された全weight_clumpを面積（幅×高さ）が大きい順にソートし、上位5つを候補として残す。

次に、その候補の中から、「リフターの中心点」に最も距離が近い2つのweight_clumpを選び出す。

軌道計算: 選び出した2つのweight_clumpの中心座標から、中間点 (mid_x, mid_y) を計算する。

この中間点の座標を、時系列リスト trajectory_coords に格納する。

補間処理: もし検出に失敗したフレームがあれば、直近の有効な座標を使って補間する（最大10フレーム程度）。

2.2 レップ（回数）カウントロジック

「軌道データのリスト（trajectory_coords）から、垂直方向（y座標）の値だけを取り出したリストに対し、移動平均による平滑化（スムージング）を行った上で、scipy.signal.find_peaks関数を使用してレップ数をカウントするロジックを実装してください。」

2.3 速度計算ロジック

「trajectory_coords（ピクセル座標）と動画のFPSを使用して、各フレーム間の速度（ピクセル/秒）を計算する関数 calculate_velocity を実装してください。
レップごとに、ピーク速度（最大速度）と平均速度を算出する処理も含めてください。」

フェーズ3：最終統合と出力

3.1 最終的な分析スクリプトの完成

「フェーズ2のロジックを全て統合した、analyze_lift_final.pyを完成させてください。

このスクリプトは、

コマンドライン引数で、モデルBのbest.ptパスと、分析したい動画のパスを受け取る。

出力パスが指定されていない場合は、入力動画名に基づいて自動生成する。

動画処理ループ内で、検出した人間のキーポイント、フィルタリングされた有効なweight_clumpのボックス、および計算した**軌跡（中間点）**をフレームに描画する。

処理終了後、ターミナルに「Total Reps: X」および「Rep N: Peak Velocity = ...」といったサマリーを出力する。

描画済みの動画をファイルとして保存する。」