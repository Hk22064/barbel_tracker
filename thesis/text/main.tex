\documentclass[dvipdfmx]{jsreport}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[dvipdfmx]{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{here}
\usepackage{amsmath}


\title{卒業論文\\深層学習を用いた\\マーカーレス・バーベル挙上速度計測システムの構築と評価}
\author{5422064 畠谷佳汰\\指導教員：中原泳青教授}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\chapter{序論 (Introduction)}

\section{研究の背景}
近年、筋力トレーニングの現場において、挙上重量（kg）だけでなく挙上速度（m/s）を指標としてトレーニング強度を管理するVelocity Based Training (VBT) の有効性が広く認められている \cite{ref:jovanovic}。速度をモニタリングすることで、その日の体調によるパフォーマンス変動を客観的に把握し、オーバートレーニングを防ぎつつ効果的な負荷設定が可能となる。
しかし、現在普及しているVBTデバイス（LPT: ニアポジショントランスデューサ等）には、導入コストが高額である（数十万円）、専用のケーブルをバーベルに取り付ける手間がかかる、動作中にケーブルが接触してフォームを阻害する可能性があるといった物理的な課題が存在する。
また、近年では「Qwik VBT」や「Metric VBT」といったスマートフォンアプリも登場し、LPTと同等の精度（r=0.90以上）が報告されている \cite{ref:balsalobre}。しかし、これらの商用アプリは独自のアルゴリズムを使用しており、その技術的詳細（バックボーンモデルの種類や平滑化処理の方法など）は公開されていないため、学術的な発展性に乏しいという課題がある。

\section{研究の目的}
本研究の目的は、誰もが所有するスマートフォン等の単眼カメラのみを用いた、安価で完全非接触なVBTシステムを構築することである。
単眼カメラ測定における最大の課題は「深さ（距離）情報の欠落」によるスケールの不定性である。これに対し、本研究ではベンチプレス競技の標準規格である「81cmライン（手幅）」に着目し、\textbf{「手首間距離」を既知の物理サイズ（0.81m）として利用するマーカーレス・キャリブレーション手法}を提案する。これにより、外部マーカーを設置することなく、実測値（m/s）に近い絶対速度の推定を可能とする。

\section{本論文の構成}
本論文は全5章から構成される。第2章では関連技術について述べる。第3章では、本研究の課題と提案手法について詳述する。第4章では、実装したシステムの評価実験と考察を行う。第5章で本研究の結論を述べる。

\chapter{準備 (Related Work)}

\section{VBTの理論}
VBTでは、挙上速度と1RM（1回挙上可能な最大重量）の間に高い相関があることを利用する。一般に、負荷が高まるにつれて挙上速度は低下し、限界（Failure）に近づくと特定の速度（Minimum Velocity Threshold）に収束する。この性質を利用し、速度低下率（Velocity Loss）をモニタリングすることで、限界までの回数（RIR: Reps In Reserve）を推定し、セットの終了タイミングを適切に判断することが可能となる。

\section{姿勢推定アルゴリズムの動向}
映像から身体の動きを捉える姿勢推定技術には、主に二つのアプローチがある。
\begin{enumerate}
    \item \textbf{Top-Downアプローチ}: YOLOv8-Pose等に代表される手法 \cite{ref:yolo, ref:yolo11}。まず画像内から「人物」を検出し、そのバウンディングボックス内でキーポイント推定を行う。
    \item \textbf{Graph-based / Heatmapアプローチ}: MediaPipe BlazePose等に代表される手法 \cite{ref:blazepose, ref:mediapipe}。骨格構造の接続関係（トポロジー）や時間的な連続性を考慮した追跡を行う。特にMediaPipeは、前フレームの推定結果を次フレームの入力として利用するトラッキング機能を有しており、動作の滑らかさとリアルタイム性の両立に優れている。
    しかし、MediaPipeを「姿勢矯正」ではなく「バーベルの速度計測」に応用し、その微分値（速度）の精度を定量的に検証した研究は未だ少ない。本研究では、このMediaPipeの追跡機能がVBTに求められる高精度な速度データ取得に適しているかを実証する点に新規性がある。
\end{enumerate}

\subsection{先行研究}
PSLeon24 \cite{ref:psleon24} は、物体検出と姿勢推定のハイブリッド構成を用いたリアルタイム姿勢フィードバックシステムを実装・公開している。同システムでは、YOLOで特定した人物領域に対してMediaPipeを適用し、\textbf{得られた骨格座標から関節角度（膝・股関節等）を算出することで、スクワットのしゃがみ深さやフォームの適正さを定量的に評価している}。本研究では、このアーキテクチャの有効性（背景ノイズへの耐性と計算効率）に着目し、同手法をVBT（Velocity Based Training）の領域へと拡張・応用することで、バーベル挙上速度の高精度な計測が可能なシステムの構築を試みた。

\chapter{本論 (Proposed Method)}

\section{問題提起と手法の選定}
VBTにおいて最も重要な指標は「速度」である。速度は位置座標の時間微分（差分）によって算出されるため、基礎となる座標データにノイズ（ジッター）が含まれている場合、微分によってそのノイズが増幅され、信頼性の高い速度データが得られない。
そこで、代表的な軽量モデルである \textbf{YOLOv11n} と \textbf{MediaPipe Pose} を対象に、同一のトレーニング動画に対する追跡性能比較（予備検討）を行った。

\subsection{座標推移とノイズ比較}
静止状態および動作中のバーベル軌跡を比較した結果、YOLOv11nはフレーム間の独立性が高いため、静止しているはずの局面でも数ピクセル単位の微細な振動（ジッター）が観測された。一方、MediaPipeは内部的なフィルタリング処理により、滑らかな軌跡が得られた。

\subsection{推論の安定性比較}
MediaPipeはフレーム間の相関を利用したトラッキングを行うため、オクルージョン（隠れ）が発生しない限り安定してキーポイントを追跡し続けることができる。対してYOLOは毎フレーム独立して検出を行うため、照明変動やブラー（ブレ）の影響を受けやすく、検出結果が断続的になる傾向が見られた。

\subsection{結果からの知見}
\begin{enumerate}
    \item \textbf{YOLO}: 検出能力は高いが、フレーム独立性が高いため速度算出におけるノイズ耐性（ジッター）に課題がある。
    \item \textbf{MediaPipe}: 時系列フィルタリングにより微分に適した滑らかなデータが得られ、VBTに必要な速度解析の精度を確保しやすい。
\end{enumerate}
VBTシステムにおいては「微分の安定性」すなわち「ノイズの少なさ」が最優先されるため、本研究では \textbf{MediaPipe} を採用することとした。

\section{研究の説明・提案システム}
本研究では、MediaPipeの滑らかさとYOLOの検出力を組み合わせたハイブリッドシステムを提案する。

\subsection{システム構成}
本システムは、Python環境上でOpenCVを用いて映像を取得し、MediaPipe Poseによりリフターの手首および膝等のキーポイントをリアルタイムに抽出する。抽出された座標データは独自のVBT解析クラスに送られ、平滑化・速度算出・イベント検知が行われる。
なお、YOLOによる検出領域を利用してMediaPipeの入力範囲を制限（Crop）するハイブリッド手法については、先行事例 \cite{ref:psleon24} を参考にした。
実際のシステム動作画面を図\ref{fig:system_view}に示す。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth, bb=0 0 1920 1080]{Thesis_Materials/figures/system_gui_view.png}
    \caption{Proposed System Interface (Analysis View)}
    \label{fig:system_view}
\end{figure}

\subsubsection{マーカーレス・キャリブレーション（独自性）}
単眼カメラ計測における最大の問題は、カメラと被写体の距離によって「1ピクセルあたりの現実の長さ（スケーリング）」が変化することである。正確な「絶対速度（m/s）」を得るには通常、既知のサイズのマーカーを画面内に配置する必要がある。
本システムでは、ベンチプレスで使用されるオリンピックシャフトの標準規格に着目した。シャフトには左右に81cm間隔のライン（81cmライン）刻まれており、多くのリフターはこのラインを目印にグリップを行う。
この特性を利用し、画像上で検出された \textbf{左右の手首（Left Wrist, Right Wrist）間のピクセル距離} を、現実空間の \textbf{0.81m} に対応させることで、スケール係数（m/px）を算出する機能を実装した。
さらに、リフターによって81cmラインに掛ける指（人差し指、中指、小指など）が異なる点を考慮し、成人男性の平均的な指幅を約2.0cmと仮定した補正機能を導入した。例えば、人差し指をラインに掛ける場合は手首が外側に広がるため、基準値に4.0cm（2.0cm $\times$ 2）を加算した0.85mとして計算を行う。
これにより、ユーザーは撮影距離やグリップスタイルを問わず、即座にm/s単位での高精度な計測を開始できる。

\subsection{疲労度モニタリング機能}
算出された絶対速度を用いて、トレーニング中の疲労管理を行う機能を実装した。
\begin{itemize}
    \item \textbf{レップ自動検知}: 速度の極大値と変曲点を監視するステートマシンにより、挙上・下降のフェーズを自動判定し、レップ数をカウントする。
\end{itemize}

\section{提案システムの詳細設計}
本節では、実装したシステムの内部ロジックについて詳述する。

\subsection{画像処理パイプライン}
本システムは、以下のハイブリッド構成により、高いロバスト性とリアルタイム性を両立している。
\begin{enumerate}
    \item \textbf{初期検出 (YOLOv11)}: まず画像全体からリフターのバウンディングボックス ($B_{box} = [x_1, y_1, x_2, y_2]$) を検出する。
    \item \textbf{動的クロッピング (Dynamic Cropping)}: 検出された領域に対し、動作によるフレームアウトを防ぐためのマージン率 $\alpha$ (本実験では 0.3) を適用した領域 $B_{crop}$ を切り出す。
    \begin{equation}
        B_{crop} = [x_1 - w \cdot \alpha, \ y_1 - h \cdot \alpha, \ x_2 + w \cdot \alpha, \ y_2 + h \cdot \alpha]
    \end{equation}
    これにより、MediaPipeへの入力解像度が向上し、遠距離撮影時の認識精度が改善される。
    \item \textbf{姿勢推定 (MediaPipe)}: 切り出された高解像度画像に対してMediaPipe Poseを適用し、33箇所のランドマーク座標 ($L_{local}$) を取得する。
    \item \textbf{座標変換}: ローカル座標系で得られた結果を、元の画像のグローバル座標系 ($L_{global}$) に再マッピングし、以後の速度計算に用いる。
\end{enumerate}

\subsection{速度算出と平滑化}
VBTにおいて最も重要な指標である「速度」は、以下の手順で算出される。

\textbf{1. グリップ幅キャリブレーション:} \\
左右の手首間距離 $d_{wrist}$ (pixel) と、既知の実測値 $W_{real}$ (0.81 m) を用いて、スケール係数 $S$ を決定する。
\begin{equation}
    S \ [m/pixel] = \frac{W_{real}}{d_{wrist}}
\end{equation}

\textbf{2. 瞬時速度の算出:} \\
時刻 $t$ における手首の垂直座標 $y_t$ と前フレーム $y_{t-1}$ の差分より、瞬時速度 $v_t$ を求める。ここで $\Delta t$ はフレーム間隔 (約0.033秒) である。
\begin{equation}
    v_t = \frac{(y_{t-1} - y_t) \times S}{\Delta t}
\end{equation}

\textbf{3. 平滑化処理 (Smoothing):} \\
生の速度データには微細なジッターが含まれるため、ウィンドウサイズ $N=5$ の移動平均フィルタを適用し、解析用速度 $\hat{v}_t$ を得る。
\begin{equation}
    \hat{v}_t = \frac{1}{N} \sum_{i=0}^{N-1} v_{t-i}
\end{equation}

\subsection{ステートマシンによる動作解析}
誤検知を防ぐため、以下の3状態を持つステートマシンを実装した。
\begin{enumerate}
    \item \textbf{WAITING}: 速度が静止状態 ($-0.05 < v < 0.05$)。
    \item \textbf{ECCENTRIC}: 速度が負の閾値 ($v < -0.05$) を下回った状態で、下降フェーズと判定する。
    \item \textbf{CONCENTRIC}: 速度が正の閾値 ($v > 0.05$) を超えた状態で、挙上フェーズと判定する。この期間のピーク速度および平均速度を記録する。
\end{enumerate}
速度が再び0近傍に戻った時点で1レップの終了とみなし、移動距離が物理的に妥当 (例: 15cm以上) である場合のみカウントを確定させる。

\chapter{考察・評価実験 (Discussion \& Evaluation)}

\section{実験環境}

本実験は以下の計算機環境にて実施した。

\begin{table}[H]
    \centering
    \caption{実験環境}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{項目} & \textbf{スペック} & \textbf{備考} \\
        \hline
        \textbf{OS} & Windows 11 Home & \\
        \hline
        \textbf{CPU} & Intel Core i7 / Ryzen 7 & MediaPipe (提案手法) の演算 \\
        \hline
        \textbf{GPU} & \textbf{NVIDIA RTX 4070 Ti} & YOLO (比較手法 A) の推論 \\
        \hline
        \textbf{Memory} & 16 GB & \\
        \hline
        \textbf{Language} & Python 3.10 & \\
        \hline
        \textbf{Libraries} & Ultralytics, MediaPipe & \\
        \hline
    \end{tabular}
\end{table}

\textbf{手法別のリソース利用特性:}
\begin{itemize}
    \item \textbf{Proposed (Hybrid):} 初期位置特定(YOLO)のみGPUを使用するが、以降の追跡(MediaPipe)は\textbf{CPUのみ}で動作する。これにより、ハイエンドなGPUを持たないスマートフォン等のエッジデバイスでも、高いFPSと低消費電力を両立可能である。
    \item \textbf{Comparison A (YOLO11x-Pose):} 毎フレーム巨大なモデル推論を行うため、高性能な\textbf{GPU (RTX 4070 Ti等)} が必須である。
\end{itemize}

\section{実験方法}
開発したシステムの、実環境におけるロバスト性（堅牢性）を検証するため、ベンチプレス動作を用いた以下の比較実験を行った。
特に、スマートフォンを用いた簡易計測において最も発生しやすい「撮影アングルの違い」が、計測精度およびロバスト性に与える影響に焦点を当てた。

\textbf{比較条件}:
\begin{enumerate}
    \item \textbf{Condition A (Standard)}: 一般的な横アングル撮影（16:9）。プレートおよびリフター全身が画角内に収まっている理想環境。
    \item \textbf{Condition B (Vertical)}: スマートフォン縦持ち撮影（9:16）。画角が狭く、プレートの一部が見切れたり、全身が映らない場合がある環境。
\end{enumerate}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.65\textwidth}
        \centering
        \includegraphics[height=5cm]{Thesis_Materials/figures/condition_horizontal_front.png}
        \caption{Condition A: Standard (Front/Horizontal)}
    \end{minipage}
    \hfill
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[height=5cm, angle=-90]{Thesis_Materials/figures/condition_vertical_far.png}
        \caption{Condition B: Vertical (Far/Smartphone)}
    \end{minipage}
\end{figure}

\textbf{評価指標}:
\begin{itemize}
    \item レップ検知成功率 (Detection Success)
    \item 条件別ロバスト性 (Robustness per Condition)
\end{itemize}

\section{実験結果}

\subsection{定量評価 (Quantitative Analysis)}

本実験では、開発したシステム（パラメータ設定: フィルタ=Average, 平滑化=5, しきい値=0.04）を用いて、3つの検証用動画に対する解析および精度評価を行った。評価指標として、レップ検出率（Detection Rate）、平均絶対誤差（MAE）、平均絶対パーセント誤差（MAPE）、および相関係数（Correlation Coeff.）を用いた。

\begin{table}[H]
    \centering
    \caption{手法別 レップ検出結果の比較}
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{動画ID (Condition)} & \textbf{正解} & \textbf{Proposed} & \textbf{Comp A} \\
        & \textbf{Reps} & \textbf{(Hybrid)} & \textbf{(YOLO11x)} \\
        \hline
        front\_9rep (Horizontal, Front) & 9 & \textbf{9 (100\%)} & 8 (89\%)\\
        \hline
        right\_9rep (Horizontal, Right) & 9 & \textbf{10 (Over)} & 10 (Over)\\
        \hline
        front\_5rep (Vertical, Medium) & 5 & \textbf{5 (100\%)} & 5 (100\%)\\
        \hline
        left\_10rep (Horizontal, Left) & 10 & \textbf{8 (80\%)} & 13 (Over)\\
        \hline
        front\_10rep (Vertical, Far) & 10 & \textbf{10 (100\%)} & 3 (30\%)\\
        \hline
    \end{tabular}
\end{table}

\textbf{結果の分析:}
\begin{enumerate}
    \item \textbf{正面・縦撮りでの優位性:} 予備実験やYOLOv11単体（比較手法A）では検出が困難であった \texttt{front\_10rep}（縦画面・被写体が遠い・高回数）において、提案手法が圧倒的な優位性（100\% vs 30\%）を示した。これは「Lock-Crop機構」による追跡の安定化と、最適化された平滑化パラメータ（Window=5）の効果である。
    \item \textbf{サイドビューの課題:} 一方で、\texttt{right\_9rep}（右側）や \texttt{left\_10rep}（左側）においては、提案手法・比較手法ともに過検知や検出漏れが発生した。これは、プレートによる身体の隠れ（オクルージョン）により、手首位置の推定精度が低下したためと考えられる。
\end{enumerate}

以上の結果を踏まえ、次節の「速度推定精度の検証」においては、正確なレップ検出に成功した \texttt{front} 系および \texttt{vertical} 系の動画3本を対象として評価を行う。

\subsection{速度計測精度の検証 (Velocity Accuracy Verification)}

システムによって算出された挙上速度（System Vel）と、動作距離（21cm）および所要フレーム数から算出した基準値（Manual Vel）の一致度を検証した。
基準値の算出方法は以下の通りである。まず、被験者のベンチプレス動作におけるボトム（胸接触位置）からトップ（肘伸展位置）までの垂直移動距離を実測し、0.21 m と特定した。次に、各レップのコンセントリック局面（挙上開始から終了まで）に要したフレーム数 $N$ を動画編集ソフトを用いて目視で計測した。動画のフレームレートは 30 fps であるため、基準となる平均挙上速度 $v_{manual}$ は以下の式で算出される。

\begin{equation}
    v_{manual} = \frac{0.21}{N \times (1/30)}
\end{equation}

全体の結果サマリを表2に示す。

\begin{table}[H]
    \centering
    \caption{速度推定精度の全体サマリ}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{指標} & \textbf{front\_5rep} & \textbf{front\_9rep} & \textbf{front\_10rep} & \textbf{全体平均} \\
        \hline
        \textbf{MAE (m/s)} & 0.009 & 0.023 & 0.040 & \textbf{0.024} \\
        \hline
        \textbf{MAPE (\%)} & 4.6\% & 11.4\% & 15.1\% & \textbf{10.4\%} \\
        \hline
        \textbf{相関係数 (r)} & 0.76 & 0.95 & 0.86 & \textbf{0.86} \\
        \hline
    \end{tabular}
\end{table}

システム全体の平均MAPEは \textbf{10.4\%} であった。この値は、先行研究 \cite{ref:balsalobre} において報告されている市販VBTアプリの精度（誤差約10\%前後）を参考値とすれば、実用上許容される範囲内にあると考えられる。また、全体平均の相関係数は \textbf{r=0.86} と高い値を示しており、本システムがリフターの挙上パフォーマンスの変動を十分に捉えていることを示唆している。

以下に、各条件ごとの詳細な分析を述べる。

\subsubsection*{(1) front\_5rep (縦撮り・中重量)}

5レップ中4レップで誤差10\%未満を達成した（MAPE: 4.6\%）。本条件はフォームが安定しているため、システムも非常に滑らかな速度曲線を抽出できた。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth, bb=0 0 750 450]{Thesis_Materials/plot_front_5rep_velocity.png}
    \caption{Velocity Comparison: front\_5rep}
\end{figure}

\subsubsection*{(2) front\_9rep (横撮り・高回数)}
後半にかけて疲労により挙上速度が低下する（0.24m/s $\to$ 0.10m/s）条件であったが、低速度域においてもシステムは高い精度を維持した。特に 8〜9 レップ目の限界付近（スティッキングポイント）において誤差が 5\% 未満に抑えられている点は、VBTの主目的である「限界判定（Failure Detection）」において本システムがきわめて有効であることを示唆している。

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth, bb=0 0 750 450]{Thesis_Materials/plot_front_9rep_velocity.png}
    \caption{Velocity Comparison: front\_9rep}
\end{figure}

\subsubsection*{(3) front\_10rep (縦撮り・遠距離)}
最も条件の厳しい動画であるが、全10レップの検出に成功した。ただし、MAEは 0.040 m/s と他の条件に比べて若干大きくなった。特に9レップ目で乖離が見られたが、これは被写体がカメラから遠いために1ピクセルあたりの解像度が低下し、手首検出のジッター影響が相対的に大きくなったためと考えられる。


\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth, bb=0 0 750 450]{Thesis_Materials/plot_front_10rep_velocity.png}
    \caption{Velocity Comparison: front\_10rep}
\end{figure}

\section{考察}

\subsection{ロバスト性の向上要因}
実験の結果、本システムは「縦撮り」「遠距離」「高回数」といった悪条件下においても、高いロバスト性を示した。この主要因は以下の2点であると考えられる。

\begin{enumerate}
    \item \textbf{ハイブリッド追跡の効果:}
    YOLOv11による初期領域の特定と、MediaPipeによる時系列追跡の組み合わせにより、単一モデルでは発生しやすい「フレームごとの検出飛び」を抑制できた。
    \item \textbf{パラメータ最適化:}
    移動平均フィルタの期間を `5フレーム`（約0.16秒）に設定したことで、MediaPipe特有の高周波ノイズを除去しつつ、挙上開始・終了の急激な速度変化に追従できたことが、検出率100\%に寄与した。
\end{enumerate}

\subsection{限界と今後の課題}
一方で、\texttt{front\_10rep} の一部で見られた精度の低下は、解像度不足に起因する。遠距離撮影時には「手首」の画素数が数ピクセル程度になるため、サブピクセル精度の推定誤差が無視できない影響を持つ。
今後の改善策として、超解像技術（Super Resolution）の前処理導入や、カルマンフィルタ等のより高度な予測フィルタの適用が挙げられる。また、本実験では手動キャリブレーション値（81cmライン）を使用したが、このラインが見えない環境下での精度検証も今後の課題である。

\chapter{結論 (Conclusion)}

本研究では、MediaPipeを用いたマーカーレスVBTシステムを構築し、その実用性を評価した。
予備実験において、MediaPipeはYOLOと比較して微細なジッターが少なく、微分を用いた速度算出に適していることを確認した。
さらに、実環境を想定したアングル比較実験により、物体検出ベースの手法が苦手とする「縦撮り・見切れ」環境においても、ハイブリッド手法（Lock-Crop）を用いることで、MediaPipeの追跡能力を最大限に引き出し、安定した計測が可能であることが示された。

以上の結果から、単眼カメラを用いた民主化されたVBTシステムを実現するための有力なアプローチの一つは、\textbf{「MediaPipeによる高速な姿勢推定」} と \textbf{「手首間距離を利用したマーカーレス自動キャリブレーション」} の組み合わせであると結論付ける。今後の展望として、スクワットやデッドリフトなど他の種目への適用や、リアルタイムでの音声フィードバック機能の実装に加えて、取得された骨格情報を活用した「フォーム評価」への拡張といった、より高度なトレーニング支援システムへの発展が期待される。

\begin{thebibliography}{99}
    \bibitem{ref:blazepose} Bazarevsky, V., et al. (2020). "BlazePose: On-device Real-time Body Pose tracking." arXiv preprint arXiv:2006.10204.
    \bibitem{ref:jovanovic} Jovanovic, M., \& Flanagan, E. P. (2014). "Researched applications of velocity based strength training." Journal of Australian Strength and Conditioning, 22(2), 58-69.
    \bibitem{ref:balsalobre} Balsalobre-Fernandez, C., et al. (2014). "The validity and reliability of an iPhone app for measuring barbell velocity." Journal of Sports Sciences, 32(16), 1570-1577.
    \bibitem{ref:yolo} Redmon, J., et al. (2016). "You Only Look Once: Unified, Real-Time Object Detection." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
    \bibitem{ref:mediapipe} Lugaresi, C., et al. (2019). "MediaPipe: A Framework for Building Perception Pipelines." arXiv preprint arXiv:1906.08172.
    \bibitem{ref:yolo11} Ultralytics. (2024). "Ultralytics YOLOv11." Available at: \url{https://github.com/ultralytics/ultralytics}.
    \bibitem{ref:psleon24} PSLeon24. (2024). "AI\_Exercise\_Pose\_Feedback." Available at: \url{https://github.com/PSLeon24/AI_Exercise_Pose_Feedback}.
\end{thebibliography}

\end{document}